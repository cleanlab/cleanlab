{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Training Entity Recognition Model for Token Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676421",
   "metadata": {},
   "source": [
    "In this tutorial, we show how you can retrieve the out-of-sample model-predicted probabilities and labels from a NLP token-classification dataset. These outputs are used to identify potential label issues in the dataset, which are demonstrated [here](https://cleanlab.ai/). The specific token classification task we consider here is Named Entity Recognition with the CoNLL-2003 dataset, and we train a Transformer network for this task using the HuggingFace transformers library. TODO: update link \n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Read and process datasets in CoNLL formats \n",
    "- Compute out-of-sample predicted probabilities via cross-validation \n",
    "- Reduce subword-level tokens to word-level tokens \n",
    "\n",
    "\\* In most NLP literatures, tokens typically refer to words or punctuation marks, while most modern tokenizers break down longer words into subwords. To avoid confusion, given tokens are referred as \"word-level tokens\", which represent the individual tokens given in the original dataset with a separate class label provided for each token; tokens obtained from the tokenizers are referred as \"subword-level tokens\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install the required dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0fb4a",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows: \n",
    "\n",
    "    !pip install tqdm transformers \n",
    "    !pip install cleanlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://data.deepai.org/conll2003.zip && mkdir data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27944bd",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# Package versions used: tqdm==4.64.0 transformers==4.22.0.dev0 cleanlab==2.0.0 numpy==1.23.0 sklearn==0.0 \n",
    "# (cont): torch==1.12.0 nltk==3.7 pytorch_transformers==1.2.0 seqeval==1.2.2 \n",
    "# ericwang/cleanlab -b token_classification for now \n",
    "\n",
    "dependencies = [\"tqdm\", \"transformers\", \"cleanlab\", \"sklearn\", \"torch\", \"nltk\", \"pytorch_transformers\", \"seqeval\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers tqdm \n",
    "import numpy as np\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from cleanlab.internal.token_classification_utils import * \n",
    "from tqdm import tqdm \n",
    "from bert import Ner \n",
    "from token_classification_tutorial_utils import * \n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "\n",
    "# Disable `TOKENIZERS_PARALLELISM` if multiple processors exist. \n",
    "import os \n",
    "if os.cpu_count() > 1: \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32604",
   "metadata": {},
   "source": [
    "## 2. Fetch the CONLL-2003 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cd5a9",
   "metadata": {},
   "source": [
    "CONLL-2003 dataset is in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "In our example, we focus on the `ner_tags` (named-entity recognition tags), which include: \n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). Here, all-caps words are casted into lowercase except for the first character (eg. `JAPAN` -> `Japan`). This is to discourage the tokenizer from breaking such words into multiple subwords. The `readfile` implementation is adapted from [here](https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py#L92). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, labels = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148277",
   "metadata": {},
   "source": [
    "`given_words` and `given_labels` are in nested list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d505f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tLabel\tEntity\n",
      "-------------------------------\n",
      "Eu              5\tB-ORG     \n",
      "rejects         0\tO         \n",
      "German          1\tB-MISC    \n",
      "call            0\tO         \n",
      "to              0\tO         \n",
      "boycott         0\tO         \n",
      "British         1\tB-MISC    \n",
      "lamb            0\tO         \n",
      ".               0\tO         \n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "\n",
    "print('Word\\t\\tLabel\\tEntity\\n-------------------------------') \n",
    "for word, label in zip(given_words[i], given_labels[i]): \n",
    "    print('{:14s}{:3d}\\t{:10s}'.format(word, label, entities[label])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8639b",
   "metadata": {},
   "source": [
    "Next, obtain the sentences with some minor pre-processing for readability. Sentences containing the `#` character are removed for simplicity, because this special character is later used to represent subword-tokens by our sentence tokenizers (See section 4 for more details). Additionally, sentences such that `len(sentence) <= 1` are also removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f19eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c496b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 20718\n",
      "Eu rejects German call to boycott British lamb.\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences: %d' % len(sentences)) \n",
    "print(sentences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1b2b0",
   "metadata": {},
   "source": [
    "## 3. Train Models using Cross-Validation \n",
    "\n",
    "To identify potential label errors in the training dataset, we compute the out-of-sample predicted probabilities using cross-validation. We first partition the dataset into k-folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797df997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'folds/' already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "lines = [[]] \n",
    "for filepath in filepaths: \n",
    "    for line in open(filepath) : \n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(lines[-1]) > 0: \n",
    "                lines.append([]) \n",
    "        else: \n",
    "            lines[-1].append(line) \n",
    "        \n",
    "lines = lines[:-1] \n",
    "lines = [line for m, line in zip(mask, lines) if m] \n",
    "\n",
    "k = 5 \n",
    "indices = create_folds(lines, k=k) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304653aa",
   "metadata": {},
   "source": [
    "We train one model for each fold's training/testing pair: \n",
    "\n",
    "- Warning! The following code will take a long time to execute, and is recommended to run on GPU, otherwise it will take forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c877f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 already exists, skipping...\n",
      "Model 1 already exists, skipping...\n",
      "Model 2 already exists, skipping...\n",
      "Model 3 already exists, skipping...\n",
      "Model 4 already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "for i in range(k): \n",
    "    if os.path.exists('folds/fold%d/model/' % i): \n",
    "        print('Model %d already exists, skipping...' % i) \n",
    "    else: \n",
    "        print('Training model on fold %d (out of %d) of cross-validation...' % (i, k)) \n",
    "        os.system(\n",
    "            \"python3 run_ner.py --data_dir=folds/fold%d --bert_model=bert-base-cased \" \\\n",
    "            \"--task_name=ner --output_dir=folds/fold%d/model --max_seq_length=256 \" \\\n",
    "            \"--do_train --num_train_epochs 10 --warmup_proportion=0.1\" % (i, i)) \n",
    "        print('Model %d saved' % i) \n",
    "        # TODO: make this a function in run_ner.py? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621481",
   "metadata": {},
   "source": [
    "## 4. Compute Out-of-Sample Predicted Probabilities \n",
    "\n",
    "We obtain the predicted probabilities for each sample using the model in which the sample was held out from training. Note that most tokenizers break down sentences into subword-level tokens, which are units smaller than word-level tokens. For example, the following sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560f8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu rejects German call to boycott British lamb.\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "print(sentences[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d0119",
   "metadata": {},
   "source": [
    "is tokenized into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08446655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '##u', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.']\n"
     ]
    }
   ],
   "source": [
    "model = Ner(\"folds/fold0/model/\") \n",
    "tokens = model.tokenize(sentences[i])[0] \n",
    "print(tokens) \n",
    "tokens = [token.replace('#', '') for token in tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493203d3",
   "metadata": {},
   "source": [
    "`##` indicates that the token is a subword. We collect both the predicted probabilities and the tokens for each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fac29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4144/4144 [01:14<00:00, 55.58it/s]\n",
      "100%|██████████| 4144/4144 [01:14<00:00, 55.37it/s]\n",
      "100%|██████████| 4144/4144 [01:14<00:00, 55.43it/s]\n",
      "100%|██████████| 4143/4143 [01:14<00:00, 55.57it/s]\n",
      "100%|██████████| 4143/4143 [01:14<00:00, 55.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens, sentence_probs = {}, {} \n",
    "for i in range(k): \n",
    "    model = Ner(\"folds/fold%d/model/\" % i) \n",
    "    for index in tqdm(indices[i]): \n",
    "        sentence_probs[index], sentence_tokens[index] = model.predict(sentences[index]) \n",
    "        \n",
    "sentence_tokens = [sentence_tokens[i] for i in range(len(sentences))] \n",
    "sentence_probs = [sentence_probs[i] for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a24f4",
   "metadata": {},
   "source": [
    "Most tokenizers partition sentences into subword-level tokens without altering the characters. However, you should verify whether any characters are modified, particularly for edge cases such as single or double quotations. In this example, double quotations `\"` are broken down into two `'`s or two `` ` ``s. Run the following to verify if any of the characters in the sentences has been modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237ff9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(modified(given_words, sentence_tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b498649",
   "metadata": {},
   "source": [
    "Given that some characters have been modified, we need to map `sentence_tokens` back to the characters from the original dataset, so that we can compare the predicted labels with the given labels. Note that such mapping is different for different models, and is not required by most tokenizers. We cannot work with modified tokens directly because we do not have the given labels for the modified tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8a5b0",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used to map `sentence_tokens` back to the original characters.</summary>\n",
    "    \n",
    "    replace = [('#', ''), ('``', '\"'), (\"''\", '\"')] \n",
    "    sentence_tokens = [[process_token(token, replace) for token in sentence_tokens[i]] for i in range(len(sentences))] \n",
    "\n",
    "    for i in range(len(sentences)): \n",
    "        short = ''.join(given_words[i]) \n",
    "        if \"''\" in short: \n",
    "            processed_tokens, processed_probs = [], [] \n",
    "            for token, prob in zip(sentence_tokens[i], sentence_probs[i]): \n",
    "                if token != '\"': \n",
    "                    processed_tokens.append(token) \n",
    "                    processed_probs.append(prob) \n",
    "                else: \n",
    "                    for _ in range(2): \n",
    "                        processed_tokens.append(\"'\") \n",
    "                        processed_probs.append(prob) \n",
    "            sentence_tokens[i] = [token for token in processed_tokens] \n",
    "            sentence_probs[i] = np.array([prob for prob in processed_probs]) </details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09fe4c5",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "replace = [('#', ''), ('``', '\"'), (\"''\", '\"')] \n",
    "sentence_tokens = [[process_token(token, replace) for token in sentence_tokens[i]] for i in range(len(sentences))] \n",
    "\n",
    "for i in range(len(sentences)): \n",
    "    short = ''.join(given_words[i]) \n",
    "    if \"''\" in short: \n",
    "        processed_tokens, processed_probs = [], [] \n",
    "        for token, prob in zip(sentence_tokens[i], sentence_probs[i]): \n",
    "            if token != '\"': \n",
    "                processed_tokens.append(token) \n",
    "                processed_probs.append(prob) \n",
    "            else: \n",
    "                for _ in range(2): \n",
    "                    processed_tokens.append(\"'\") \n",
    "                    processed_probs.append(prob) \n",
    "        sentence_tokens[i] = [token for token in processed_tokens] \n",
    "        sentence_probs[i] = np.array([prob for prob in processed_probs]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edbe08",
   "metadata": {},
   "source": [
    "In this example, we are more interested in severe types of mislabels, such as `B-LOC` vs. `B-PER`, instead of `B-LOC` vs. `I-LOC`. Therefore, we discard the `B-` and `I-` prefixes, and get the model-predicted probabilities for each subword-level token. The merged entities are `[O, MIS, PER, ORG, LOC]`, which correspond to the classes in our token classification task. In a [subsequent notebook](https://cleanlab.ai/), we will use the probabilistic predictions from this trained model to identify instances where the class label was incorrectly chosen for particular tokens. As shown below: TODO: update link \n",
    "\n",
    "- `given_maps` is an array of length equal to the original number of entities, such that `given_maps[i]` is the mapped entity of the i'th entity \n",
    "- `model_maps` is an array of length equal to the number of model predicted labels, such that `model_maps[i]` is the mapped entity of the i'th model predicted entity. If `model_maps[i] < 0`, it indicates that the entity does not map to a valid named entity. This usually occurs when the model predicted entities include start/end tags. If `np.any(model_maps < 0)`, `pred_probs` will be normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2cc02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "model_maps = [-1, 0, 1, 1, 2, 2, 3, 3, 4, 4, -1, -1] \n",
    "given_labels = [mapping(labels, maps=given_maps) for labels in given_labels] \n",
    "sentence_probs = [merge_probs(pred_prob, maps=model_maps) for pred_prob in sentence_probs] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16f9cf",
   "metadata": {},
   "source": [
    "Specifically, `merge_probs` takes in two parameters: \n",
    "\n",
    "- `probs`: `np.array` of shape `(N, L)`, where `N` is the number of tokens in the sentence, and `L` is the number of classes of the model. \n",
    "- `maps`: `list` of length `L`, where `L` is the number of classes of the model, with details specified above in `model_maps`. \n",
    "\n",
    "and returns: \n",
    "\n",
    "- `probs_merged`: `np.array` of shape `(N, K)`, where `N` is the number of tokens in the sentence, and `K` is the number of classes of the new set of entities. \n",
    "\n",
    "such that `probs_merged[:, j] == \\sum_{maps[j']=j} probs[:, j']`. If any element in `maps` is negative (does not map to anything in the new set of classes), `probs_merged` is normalized such that each row sums up to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3baf",
   "metadata": {},
   "source": [
    "## 5. Reducing from subword-level to word-level \n",
    "\n",
    "When a sentence gets tokenized, each word-level token may be broken down into subword-level tokens, each of which generates a predicted probability. Given that we only have the labels for word-level tokens, we reduce the subword-level tokens to word-level tokens. \n",
    "\n",
    "\\* For this example, most subwords-to-words reduction are handled internally, but for most other models the reduction has to be done manually. In the following, we show our method of reduction, which is slightly different from how the `bert` model reduces it. See [here](https://github.com/kamalkraj/BERT-NER/blob/dev/bert.py#L85) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43366dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tEu rejects German call to boycott British lamb.\n",
      "Given words:\t['Eu', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Subwords:\t['E', 'u', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', 'mb', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:\\t' + sentences[0]) \n",
    "print('Given words:\\t' + str(given_words[0])) \n",
    "print('Subwords:\\t' + str(tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eccd57",
   "metadata": {},
   "source": [
    "The word `lamb` is tokenized into two subwords `la` and `mb`. In this case, we assign the average predicted probabilities of the two subwords to the token. Alternatively, we can take the weighted average, such that the weight for each predicted probability is proportional to the length of its corresponding subword. This is to ensure that longer subwords have heavier weights on the average predicted probabilities, although the benefits are insignificant for most datasets. \n",
    "\n",
    "Each tokenizer tokenizes sentences differently. In some rare cases, a subword may overlap two tokens, resulting in a misalignment in tokenization. For example, consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0565e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\tMassachusetts Institute of Technology (MIT)\n",
      "Given words:\t['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')']\n",
      "Subwords:\t['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')']\n"
     ]
    }
   ],
   "source": [
    "demo_sentence = 'Massachusetts Institute of Technology (MIT)' \n",
    "demo_given_words = ['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')'] \n",
    "demo_subwords = ['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')'] \n",
    "\n",
    "print('Sentence:\\t' + demo_sentence) \n",
    "print('Given words:\\t' + str(demo_given_words)) \n",
    "print('Subwords:\\t' + str(demo_subwords)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f3d0",
   "metadata": {},
   "source": [
    "In this case, we assign the predicted probabilities of `(M` to `(`, and the average predicted probabilities of `(M` and `IT` to `MIT`. \n",
    "\n",
    "We use the method above to map the predicted probabilities for each token generated by the tokenizers to each token in the original dataset. `get_pred_probs` return `pred_probs` which is a nested list, such that `pred_probs[i]` is a `np.array` of shape `(N, K)`, such that `N` is the number of given tokens for sentence `i`, and `K` is the number of classes. This is the expected format for methods in the `cleanlab.token_classification` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3656f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = [get_pred_probs(sentence_probs[i], sentence_tokens[i], given_words[i]) \n",
    "                         for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c583d",
   "metadata": {},
   "source": [
    "For example, we observe the tokens, given labels of the first sentence, and its predicted probabilities and label for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2f2c113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Eu, given label: ORG\n",
      "Predicted probabilities: [3.041000e-04 2.383000e-04 9.993621e-01 7.010000e-05 2.550000e-05]\n",
      "Predicted label: PER\n",
      "\n",
      "Token: rejects, given label: O\n",
      "Predicted probabilities: [9.99988e-01 4.00000e-06 2.20000e-06 4.50000e-06 1.30000e-06]\n",
      "Predicted label: O\n",
      "\n",
      "Token: German, given label: MISC\n",
      "Predicted probabilities: [7.500000e-06 9.999611e-01 1.370000e-05 8.700000e-06 9.000000e-06]\n",
      "Predicted label: MISC\n",
      "\n",
      "Token: call, given label: O\n",
      "Predicted probabilities: [9.999894e-01 3.800000e-06 1.800000e-06 3.700000e-06 1.400000e-06]\n",
      "Predicted label: O\n",
      "\n",
      "Token: to, given label: O\n",
      "Predicted probabilities: [9.99991e-01 2.70000e-06 1.70000e-06 3.50000e-06 1.10000e-06]\n",
      "Predicted label: O\n",
      "\n",
      "Token: boycott, given label: O\n",
      "Predicted probabilities: [9.999877e-01 4.800000e-06 2.000000e-06 4.400000e-06 1.100000e-06]\n",
      "Predicted label: O\n",
      "\n",
      "Token: British, given label: MISC\n",
      "Predicted probabilities: [4.700000e-06 9.999639e-01 1.100000e-05 1.160000e-05 8.800000e-06]\n",
      "Predicted label: MISC\n",
      "\n",
      "Token: lamb, given label: O\n",
      "Predicted probabilities: [9.999867e-01 3.600000e-06 2.100000e-06 4.700000e-06 2.800000e-06]\n",
      "Predicted label: O\n",
      "\n",
      "Token: ., given label: O\n",
      "Predicted probabilities: [9.999907e-01 2.100000e-06 1.600000e-06 4.400000e-06 1.100000e-06]\n",
      "Predicted label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "for word, label, prob in zip(given_words[0], given_labels[0], pred_probs[0]): \n",
    "    print('Token: %s, given label: %s' % (word, entities[label])) \n",
    "    print('Predicted probabilities: %s' % str(np.round(prob, 7))) \n",
    "    print('Predicted label: %s\\n' % entities[np.argmax(prob)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30126",
   "metadata": {},
   "source": [
    "## 6. Save `pred_probs` \n",
    "\n",
    "Lastly, we save the predicted probabilities and given labels. `to_dict` converts `pred_probs` into a dictionary `d` where `d[str(i)]==pred_probs[i]`. The dictionary is saved as a `.npz` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3066803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_dict = to_dict(pred_probs) \n",
    "np.savez('pred_probs.npz', **pred_probs_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2d51a",
   "metadata": {},
   "source": [
    "## 7. Model evaluation  \n",
    "\n",
    "Lastly, we evaluate the model accuracy. We use the definition of precision and recall introduced by CoNLL-2003: \n",
    "\n",
    "> *“precision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file.”*\n",
    "\n",
    "See [here](https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d285325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t\t0.949\n",
      "Recall\t\t\t0.952\n",
      "f1-score\t\t0.951\n",
      "Accuracy\t\t0.989\n",
      "Balanced Accuracy\t0.955\n"
     ]
    }
   ],
   "source": [
    "predictions = [pred_prob.argmax(axis=1) for pred_prob in pred_probs] \n",
    "predictions_flatten = [pred for prediction in predictions for pred in prediction] \n",
    "given_labels_flatten = [label for given_label in given_labels for label in given_label] \n",
    "\n",
    "counts = [0, 0, 0, 0] \n",
    "correct = 0 \n",
    "\n",
    "for truth, prediction in zip(given_labels_flatten, predictions_flatten): \n",
    "    if truth != 0: \n",
    "        if truth == prediction: \n",
    "            counts[0] += 1 \n",
    "        counts[1] += 1 \n",
    "    if prediction != 0: \n",
    "        if truth == prediction: \n",
    "            counts[2] += 1 \n",
    "        counts[3] += 1 \n",
    "    if truth == prediction: \n",
    "        correct += 1 \n",
    "        \n",
    "precision = counts[2] / counts[3] \n",
    "recall = counts[0] / counts[1] \n",
    "f1 = 2 * precision * recall / (precision + recall) \n",
    "accuracy = correct / len(given_labels_flatten) \n",
    "\n",
    "balanced_accuracy = balanced_accuracy_score(given_labels_flatten, predictions_flatten) \n",
    "\n",
    "print('Precision\\t\\t%.3f\\nRecall\\t\\t\\t%.3f\\nf1-score\\t\\t%.3f\\nAccuracy\\t\\t%.3f\\nBalanced Accuracy\\t%.3f' % \n",
    "     (precision, recall, f1, accuracy, balanced_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ab910c",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "expected_words = ['Eu', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] \n",
    "expected_labels = [3, 0, 1, 0, 0, 0, 1, 0, 0] \n",
    "if given_words[0] != expected_words or given_labels[0] != expected_labels: \n",
    "    raise Exception(\"Something wrong with reading file\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
