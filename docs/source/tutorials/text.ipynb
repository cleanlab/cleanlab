{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TensorFlow, Keras, and Cleanlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 5-minute quickstart tutorial, we use cleanlab to find potential label errors in a text classification dataset of [IMDB movie reviews](https://ai.stanford.edu/~amaas/data/sentiment/). This dataset contains 50,000 text reviews, each labeled with a binary sentiment polarity label indicating whether the review is positive (1) or negative (0). cleanlab will shortlist _hundreds_ of examples that confuse our ML model the most; many of which are potential label errors, edge cases, or otherwise ambiguous examples.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple TensorFlow & Keras neural net and wrap it with cleanlab's `KerasWrapperSequential`, a Keras model wrapper that is compatible with cleanlab's `CleanLearning` and scikit-learn.\n",
    "\n",
    "- Use `CleanLearning` to automatically compute out-of-sample preddicted probabilites and identify potential label errors with the `find_label_issues` method.\n",
    "\n",
    "- Train a more robust version of the same neural net after dropping the identified label errors using `CleanLearning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Quickstart\n",
    "<br/>\n",
    "    \n",
    "Already have an sklearn compatible `model`, `data` and given `labels`? Run the code below to train your `model` and get label issues using `CleanLearning`. \n",
    "    \n",
    "You can subsequently use the same `CleanLearning` object to train a more robust model (only trained on the clean data) by calling the `.fit()` method and passing in the `label_issues` found earlier.\n",
    "\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```python\n",
    "\n",
    "from cleanlab.classification import CleanLearning\n",
    "\n",
    "cl = CleanLearning(model)\n",
    "label_issues = cl.find_label_issues()  # label issues identified\n",
    "  \n",
    "cl.fit(train_data, labels, label_issues=label_issues)\n",
    "preds = cl.predict(test_data)  # predictions from a version of your model \n",
    "                               # trained on auto-cleaned data\n",
    "\n",
    "\n",
    "```\n",
    "    \n",
    "</div>\n",
    "    \n",
    "Is your model/data not compatible with `CleanLearning`? You can instead run cross-validation on your model to get out-of-sample `pred_probs`. Then run the code below to get label issue indices ranked by their inferred severity.\n",
    "\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```python\n",
    "\n",
    "from cleanlab.filter import find_label_issues\n",
    "\n",
    "ranked_label_issues = find_label_issues(\n",
    "    labels,\n",
    "    pred_probs,\n",
    "    return_indices_ranked_by=\"self_confidence\",\n",
    ")\n",
    "    \n",
    "\n",
    "```\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install required dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install sklearn tensorflow tensorflow-datasets scikeras\n",
    "!pip install cleanlab\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs.cleanlab.ai).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "# Package versions we used: tensorflow==2.9.1 scikeras==0.9.0 scikit-learn==1.1.3 tensorflow_datasets==4.5.2\n",
    "\n",
    "dependencies = [\"cleanlab\", \"sklearn\", \"tensorflow\", \"tensorflow_datasets\", \"scikeras\"]\n",
    "\n",
    "# Supress outputs that may appear if tensorflow happens to be improperly installed: \n",
    "import os \n",
    "import logging \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # suppress tensorflow log output \n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL) \n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score, log_loss \n",
    "from sklearn.model_selection import cross_val_predict \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers \n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "from cleanlab.classification import CleanLearning\n",
    "from cleanlab.experimental.keras import KerasWrapperSequential\n",
    "\n",
    "SEED = 123456  # for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# This cell is hidden from docs.cleanlab.ai \n",
    "\n",
    "import random \n",
    "import numpy as np \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the IMDb text dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is provided in TensorFlow's Datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "raw_train_ds = tfds.load(name=\"imdb_reviews\", split=\"train\", batch_size=-1, as_supervised=True)\n",
    "raw_test_ds = tfds.load(name=\"imdb_reviews\", split=\"test\", batch_size=-1, as_supervised=True)\n",
    "\n",
    "raw_train_texts, train_labels = tfds.as_numpy(raw_train_ds)\n",
    "raw_test_texts, test_labels = tfds.as_numpy(raw_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(train_labels))\n",
    "print(f\"Classes: {set(train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first example in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Label: 0\n",
      "Example Text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Example Label: {train_labels[i]}\")\n",
    "print(f\"Example Text: {raw_train_texts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as two numpy arrays for each the train and test set:\n",
    "\n",
    "1. `raw_train_texts` and `raw_test_texts` for the movie reviews in text format,\n",
    "2. `train_labels` and `test_labels` for the labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "You can easily replace the above with your own text dataset, and continue with the rest of the tutorial.\n",
    "\n",
    "Your classes (and entries of `train_labels` / `test_labels`) should be represented as integer indices 0, 1, ..., num_classes - 1.\n",
    "For example, if your dataset has 7 examples from 3 classes, `train_labels` might be: `np.array([2,0,0,1,2,0,1])`\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to preprocess the text data by:\n",
    "\n",
    "1. Converting it to lower case\n",
    "2. Removing the HTML break tags: `<br />`\n",
    "3. Removing any punctuation marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `TextVectorization` layer to preprocess, tokenize, and vectorize our text data, thus making it suitable as input for a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=preprocess_text,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting `vectorize_layer` to the text data creates a mapping of each token (i.e. word) to an integer index. Note that we only adapt the vectorization on the train set, as it is standard ML practice. \n",
    "\n",
    "Subsequently, we can vectorize our text data in the train and test sets by using this mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.reset_state()\n",
    "vectorize_layer.adapt(raw_train_texts)\n",
    "\n",
    "train_texts = vectorize_layer(raw_train_texts)\n",
    "test_texts = vectorize_layer(raw_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a classification model and use cleanlab to find potential label errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a simple neural network for classification with TensorFlow and Keras. We will also wrap it with cleanlab's `KerasWrapperSequential` to make it compatible with `CleanLearning`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model():\n",
    "    model = KerasWrapperSequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
    "            layers.Embedding(max_features + 1, 16),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(num_classes),\n",
    "            layers.Softmax()\n",
    "        ], # outputs probability that text belongs to class 1\n",
    "        compile_kwargs= {\n",
    "          \"optimizer\":\"adam\",\n",
    "          \"loss\":tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          \"metrics\":tf.keras.metrics.CategoricalAccuracy(),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we need to format the data into a tensorflow dataset format (as required by the KerasWrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(buffer_size=len(train_texts)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define the `CleanLearning` object with the neural network model and using `find_label_issues` to identify potential label errors.\n",
    "\n",
    "`CleanLearning` provides a wrapper class that can easily be applied to any scikit-learn compatible model. Once wrapped, the resulting model can still be used in the exact same manner, but it will now train more robustly if the data have noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_n_folds = 3  # for efficiency; values like 5 or 10 will generally work better\n",
    "model = get_nn_model()\n",
    "cl = CleanLearning(model, cv_n_folds=cv_n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "521/521 [==============================] - 7s 12ms/step - loss: 0.6581 - categorical_accuracy: 0.4566\n",
      "Epoch 2/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.5169 - categorical_accuracy: 0.4852\n",
      "Epoch 3/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.4059 - categorical_accuracy: 0.4912\n",
      "Epoch 4/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3422 - categorical_accuracy: 0.4919\n",
      "Epoch 5/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3024 - categorical_accuracy: 0.4936\n",
      "Epoch 6/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2728 - categorical_accuracy: 0.4945\n",
      "Epoch 7/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2494 - categorical_accuracy: 0.4949\n",
      "Epoch 8/10\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 0.2286 - categorical_accuracy: 0.4939\n",
      "Epoch 9/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2121 - categorical_accuracy: 0.4954\n",
      "Epoch 10/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.1976 - categorical_accuracy: 0.4958\n",
      "261/261 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "521/521 [==============================] - 8s 14ms/step - loss: 0.6550 - categorical_accuracy: 0.5315\n",
      "Epoch 2/10\n",
      "521/521 [==============================] - 6s 10ms/step - loss: 0.5185 - categorical_accuracy: 0.4813\n",
      "Epoch 3/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.4094 - categorical_accuracy: 0.4890\n",
      "Epoch 4/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3454 - categorical_accuracy: 0.4945\n",
      "Epoch 5/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3055 - categorical_accuracy: 0.4951\n",
      "Epoch 6/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2741 - categorical_accuracy: 0.4949\n",
      "Epoch 7/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2510 - categorical_accuracy: 0.4959\n",
      "Epoch 8/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2325 - categorical_accuracy: 0.4948\n",
      "Epoch 9/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2146 - categorical_accuracy: 0.4957\n",
      "Epoch 10/10\n",
      "521/521 [==============================] - 5s 8ms/step - loss: 0.2007 - categorical_accuracy: 0.4951\n",
      "261/261 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "521/521 [==============================] - 10s 18ms/step - loss: 0.6591 - categorical_accuracy: 0.4678\n",
      "Epoch 2/10\n",
      "521/521 [==============================] - 6s 11ms/step - loss: 0.5176 - categorical_accuracy: 0.4846\n",
      "Epoch 3/10\n",
      "521/521 [==============================] - 6s 11ms/step - loss: 0.4047 - categorical_accuracy: 0.4931\n",
      "Epoch 4/10\n",
      "521/521 [==============================] - 5s 10ms/step - loss: 0.3412 - categorical_accuracy: 0.4928\n",
      "Epoch 5/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.3014 - categorical_accuracy: 0.4953\n",
      "Epoch 6/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2715 - categorical_accuracy: 0.4967\n",
      "Epoch 7/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2478 - categorical_accuracy: 0.4980\n",
      "Epoch 8/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2293 - categorical_accuracy: 0.4963\n",
      "Epoch 9/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.2120 - categorical_accuracy: 0.4964\n",
      "Epoch 10/10\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 0.1971 - categorical_accuracy: 0.4969\n",
      "261/261 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "label_issues = cl.find_label_issues(X=train_tf_dataset, labels=train_labels, clf_kwargs={\"epochs\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_label_issues` method above will perform cross validation to compute out-of-sample predicted probabilites for each examples, which is used to identify label errors.\n",
    "\n",
    "This method will return a dataframe that contains the label quality score for each example. The label quality scores are a float between 0 and 1, where a lower score indicate that an example is more likely to be mislabeled. The dataframe also contains a boolean column which specifies if that example is identified as a label error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>given_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.729744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.710710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.333053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.714980</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.498250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_label_issue  label_quality  given_label  predicted_label\n",
       "0           False       0.729744            0                0\n",
       "1           False       0.710710            0                0\n",
       "2            True       0.333053            0                1\n",
       "3           False       0.714980            1                1\n",
       "4           False       0.498250            1                0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then obtain the subset of identified label errors, and find the index of the top 10 label errors by sorting the label quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_issues = label_issues[label_issues[\"is_label_issue\"] == True]\n",
    "lowest_quality_labels = label_issues[\"label_quality\"].argsort()[:10].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanlab found 1287 potential label errors.\n",
      "Here are indices of the top 10 most likely errors: \n",
      " [ 5204 22294 21889 15079 15174 18928 10676 10589 19639  8853]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"cleanlab found {len(identified_issues)} potential label errors.\\n\"\n",
    "    f\"Here are indices of the top 10 most likely errors: \\n {lowest_quality_labels}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some of the most likely label errors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us inspect these datapoints, we define a method to print any example from the dataset. We then display some of the top-ranked label issues identified by `cleanlab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_as_df(index):\n",
    "    return pd.DataFrame(\n",
    "        {\"texts\": raw_train_texts[index], \"labels\": train_labels[index]},\n",
    "        [index]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...film seems **cheap**.\"\n",
    ">\n",
    "> - \"...unbelievably **bad**...\"\n",
    ">\n",
    "> - \"...cinematography is **badly** lit...\"\n",
    ">\n",
    "> - \"...everything looking **grainy** and **ugly**.\"\n",
    ">\n",
    "> - \"...sound is so **terrible**...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   texts  \\\n",
       "5204  b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'   \n",
       "\n",
       "      labels  \n",
       "5204       1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(5204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...incredibly **awful** score...\"\n",
    ">\n",
    "> - \"...**worst** Foley work ever done.\"\n",
    ">\n",
    "> - \"...script is **incomprehensible**...\"\n",
    ">\n",
    "> - \"...editing is just **bizarre**.\"\n",
    ">\n",
    "> - \"...**atrocious** pan and scan...\"\n",
    ">\n",
    "> - \"...**incoherent mess**...\"\n",
    ">\n",
    "> - \"...**amateur** directing there.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22294</th>\n",
       "      <td>b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.&lt;br /&gt;&lt;br /&gt;The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.&lt;br /&gt;&lt;br /&gt;The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.&lt;br /&gt;&lt;br /&gt;The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) &lt;br /&gt;&lt;br /&gt;This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.&lt;br /&gt;&lt;br /&gt;Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.&lt;br /&gt;&lt;br /&gt;Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            texts  \\\n",
       "22294  b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.<br /><br />The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.<br /><br />The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.<br /><br />The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) <br /><br />This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.<br /><br />Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.<br /><br />Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'   \n",
       "\n",
       "       labels  \n",
       "22294       1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(22294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...hard to imagine a **boring** shark movie...\"\n",
    ">\n",
    "> - \"**Poor focus** in some scenes made the production seems **amateurish**.\"\n",
    ">\n",
    "> - \"...**do nothing** to take advantage of...\"\n",
    ">\n",
    "> - \"...**far too few** scenes of any depth or variety.\"\n",
    ">\n",
    "> - \"...just **look flat**...no contrast of depth...\"\n",
    ">\n",
    "> - \"...**introspective** and **dull**...constant **disappointment**.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      texts  \\\n",
       "15079  b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'   \n",
       "\n",
       "       labels  \n",
       "15079       1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(15079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleanlab has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix these label issues or remove ambiguous examples from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a more robust model from noisy labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the label issues manually may be time-consuming, but cleanlab can filter these noisy examples and train a model on the remaining clean data for you automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish a baseline, let's first train and evaluate our original neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6219 - categorical_accuracy: 0.4861\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.4387 - categorical_accuracy: 0.4864\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.3443 - categorical_accuracy: 0.4921\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2959 - categorical_accuracy: 0.4948\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2649 - categorical_accuracy: 0.4940\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.2425 - categorical_accuracy: 0.4961\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.2241 - categorical_accuracy: 0.4944\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.2071 - categorical_accuracy: 0.4956\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.1938 - categorical_accuracy: 0.4953\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.1819 - categorical_accuracy: 0.4952\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.1712 - categorical_accuracy: 0.4963\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.1612 - categorical_accuracy: 0.4980\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.1519 - categorical_accuracy: 0.4970\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.1434 - categorical_accuracy: 0.4995\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.1363 - categorical_accuracy: 0.4980\n"
     ]
    }
   ],
   "source": [
    "baseline_model = get_nn_model()  # note we first re-instantiate the model\n",
    "baseline_model.fit(X=train_tf_dataset, y=train_labels, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step\n",
      "\n",
      " Test accuracy of original neural net: 0.86444\n"
     ]
    }
   ],
   "source": [
    "preds = baseline_model.predict(test_texts)\n",
    "acc_og = accuracy_score(test_labels, preds)\n",
    "print(f\"\\n Test accuracy of original neural net: {acc_og}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "742/742 [==============================] - 13s 17ms/step - loss: 0.6131 - categorical_accuracy: 0.5037\n",
      "Epoch 2/15\n",
      "742/742 [==============================] - 9s 11ms/step - loss: 0.3958 - categorical_accuracy: 0.4870\n",
      "Epoch 3/15\n",
      "742/742 [==============================] - 8s 11ms/step - loss: 0.2745 - categorical_accuracy: 0.4928\n",
      "Epoch 4/15\n",
      "742/742 [==============================] - 8s 10ms/step - loss: 0.2126 - categorical_accuracy: 0.4949\n",
      "Epoch 5/15\n",
      "742/742 [==============================] - 8s 10ms/step - loss: 0.1732 - categorical_accuracy: 0.4951\n",
      "Epoch 6/15\n",
      "742/742 [==============================] - 7s 9ms/step - loss: 0.1444 - categorical_accuracy: 0.4959\n",
      "Epoch 7/15\n",
      "742/742 [==============================] - 7s 9ms/step - loss: 0.1223 - categorical_accuracy: 0.4971\n",
      "Epoch 8/15\n",
      "742/742 [==============================] - 7s 9ms/step - loss: 0.1059 - categorical_accuracy: 0.4966\n",
      "Epoch 9/15\n",
      "742/742 [==============================] - 7s 9ms/step - loss: 0.0911 - categorical_accuracy: 0.4977\n",
      "Epoch 10/15\n",
      "742/742 [==============================] - 7s 9ms/step - loss: 0.0802 - categorical_accuracy: 0.4972\n",
      "Epoch 11/15\n",
      "742/742 [==============================] - 7s 8ms/step - loss: 0.0708 - categorical_accuracy: 0.4983\n",
      "Epoch 12/15\n",
      "742/742 [==============================] - 6s 8ms/step - loss: 0.0620 - categorical_accuracy: 0.4979\n",
      "Epoch 13/15\n",
      "742/742 [==============================] - 7s 8ms/step - loss: 0.0550 - categorical_accuracy: 0.4978\n",
      "Epoch 14/15\n",
      "742/742 [==============================] - 6s 8ms/step - loss: 0.0491 - categorical_accuracy: 0.4982\n",
      "Epoch 15/15\n",
      "742/742 [==============================] - 6s 8ms/step - loss: 0.0441 - categorical_accuracy: 0.4988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CleanLearning(clf=&lt;cleanlab.experimental.keras.KerasWrapperSequential object at 0x17bb91840&gt;,\n",
       "              cv_n_folds=3,\n",
       "              find_label_issues_kwargs={&#x27;min_examples_per_class&#x27;: 10})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CleanLearning</label><div class=\"sk-toggleable__content\"><pre>CleanLearning(clf=&lt;cleanlab.experimental.keras.KerasWrapperSequential object at 0x17bb91840&gt;,\n",
       "              cv_n_folds=3,\n",
       "              find_label_issues_kwargs={&#x27;min_examples_per_class&#x27;: 10})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: KerasWrapperSequential</label><div class=\"sk-toggleable__content\"><pre>&lt;cleanlab.experimental.keras.KerasWrapperSequential object at 0x17bb91840&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasWrapperSequential</label><div class=\"sk-toggleable__content\"><pre>&lt;cleanlab.experimental.keras.KerasWrapperSequential object at 0x17bb91840&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "CleanLearning(clf=<cleanlab.experimental.keras.KerasWrapperSequential object at 0x17bb91840>,\n",
       "              cv_n_folds=3,\n",
       "              find_label_issues_kwargs={'min_examples_per_class': 10})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fit(X=train_tf_dataset, labels=train_labels, label_issues=cl.get_label_issues(), clf_kwargs={\"epochs\": 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step\n",
      "Test accuracy of cleanlab's neural net: 0.8736\n"
     ]
    }
   ],
   "source": [
    "pred_labels = cl.predict(test_texts)\n",
    "acc_cl = accuracy_score(test_labels, pred_labels)\n",
    "print(f\"Test accuracy of cleanlab's neural net: {acc_cl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleanlab provides a wrapper class that can easily be applied to any scikit-learn compatible model. Once wrapped, the resulting model can still be used in the exact same manner, but it will now train more robustly if the data have noisy labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.classification import CleanLearning\n",
    "\n",
    "model = KerasClassifier(get_net(), epochs=10)  # Note we first re-instantiate the model\n",
    "cl = CleanLearning(clf=model, seed=SEED)  # cl has same methods/attributes as model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the cleanlab-wrapped model, the following operations take place: The original model is trained in a cross-validated fashion to produce out-of-sample predicted probabilities. Then, these predicted probabilities are used to identify label issues, which are then removed from the dataset. Finally, the original model is trained once more on the remaining clean subset of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cl.fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get predictions from the resulting cleanlab model and evaluate them, just like we did for our original neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test set accuracy slightly improved as a result of the data cleaning. Note that this will not always be the case, especially when we are evaluating on test data that are themselves noisy. The best practice is to run cleanlab to identify potential label issues and then manually review them, before blindly trusting any accuracy metrics. In particular, the most effort should be made to ensure high-quality test data, which is supposed to reflect the expected performance of our model during deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "highlighted_indices = [5204, 22294, 15079]  # check these examples were found in find_label_issues\n",
    "if not all(x in ranked_label_issues for x in highlighted_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from ranked_label_issues.\")\n",
    "\n",
    "# Also check that cleanlab has improved prediction accuracy\n",
    "if acc_og >= acc_cl:\n",
    "    raise Exception(\"Cleanlab training failed to improve model accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text x TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
