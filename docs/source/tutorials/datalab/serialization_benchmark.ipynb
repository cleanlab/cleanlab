{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest scikit-learn tensorflow numpy pandas cleanlab[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: New Secure Serialization vs. Legacy Pickle\n",
    "\n",
    "This notebook provides a quantitative comparison of the legacy `pickle`-based serialization engine and the new, secure engine built on **Apache Parquet** and **JSON**.\n",
    "\n",
    "We will perform two key measurements:\n",
    "1.  **Execution Time:** How long does it take to `save()` and `load()` a large `Datalab` object?\n",
    "2.  **Size on Disk:** How much storage space do the resulting artifacts consume?\n",
    "\n",
    "The goal is to provide hard data demonstrating that the new engine is not only safer but also significantly more performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll import the necessary libraries and create a large, realistic `Datalab` object to serve as our test subject. This object will contain one million rows and several issue columns, simulating a real-world use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "from cleanlab.datalab.datalab import Datalab\n",
    "\n",
    "print(\"Setting up a large Datalab object for benchmarking...\")\n",
    "\n",
    "# --- ⚠️ PERFORMANCE WARNING ---\n",
    "# The NUM_ROWS variable controls the size of the dataset and has a\n",
    "# major impact on the runtime of the `lab.find_issues()` step.\n",
    "#\n",
    "# - Small values (~10,000 rows): The process is very fast (~3 seconds)\n",
    "#   because the dataset and the algorithm's internal data structures\n",
    "#   fit within the CPU's fast cache memory.\n",
    "#\n",
    "# - Large values (>100,000 rows): The process becomes much slower\n",
    "#   (20-50 minutes) because the data overflows the CPU cache and must be\n",
    "#   processed from slower main RAM. This \"performance cliff\" is expected\n",
    "#   behavior for the O(n log n) KNN algorithm at this scale.\n",
    "#\n",
    "# The default is set low for a quick and interactive initial run.\n",
    "# To reproduce the final performance report, you must use a large value\n",
    "# and be prepared for a significant wait.\n",
    "# ---\n",
    "NUM_ROWS: int = 100_000\n",
    "# NUM_ROWS: int = 1_000_000 # Uncomment for the final benchmark run.\n",
    "\n",
    "NUM_FEATURES: int = 20\n",
    "NUM_CLASSES: int = 10\n",
    "\n",
    "# Generate synthetic data\n",
    "features: Dict[str, np.ndarray] = {f\"feature_{i}\": np.random.rand(NUM_ROWS) for i in range(NUM_FEATURES)}\n",
    "labels: np.ndarray = np.random.randint(0, NUM_CLASSES, size=NUM_ROWS)\n",
    "data: Dict[str, Any] = {**features, \"label\": labels}\n",
    "\n",
    "# Create the Datalab and find some issues to make it non-trivial\n",
    "lab: Datalab = Datalab(data=data, label_name=\"label\")\n",
    "# This is the long-running step. Let it complete once.\n",
    "lab.find_issues(pred_probs=np.random.rand(NUM_ROWS, NUM_CLASSES))\n",
    "\n",
    "print(f\"Setup complete. Datalab has {len(lab.labels):,} rows and {len(lab.issues.columns)} issue columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Benchmark Functions\n",
    "\n",
    "Next, we'll define two functions to encapsulate the benchmarking logic for each serialization method. This ensures we measure both `save` and `load` operations consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_legacy_pickle(datalab: Datalab, path: Path) -> Dict[str, Union[str, float]]:\n",
    "    \"\"\"Benchmarks the old pickle save/load method.\"\"\"\n",
    "    if path.exists():\n",
    "        shutil.rmtree(path)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pkl_path: Path = path / \"datalab.pkl\"\n",
    "\n",
    "    # Time the save operation\n",
    "    start_save: float = time.perf_counter()\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(datalab, f)\n",
    "    end_save: float = time.perf_counter()\n",
    "\n",
    "    # Time the load operation\n",
    "    start_load: float = time.perf_counter()\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        _ = pickle.load(f)\n",
    "    end_load: float = time.perf_counter()\n",
    "\n",
    "    # Measure file size\n",
    "    size_mb: float = pkl_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "    return {\n",
    "        \"Format\": \"Legacy (pickle)\",\n",
    "        \"Save Time (s)\": end_save - start_save,\n",
    "        \"Load Time (s)\": end_load - start_load,\n",
    "        \"Size on Disk (MB)\": size_mb,\n",
    "    }\n",
    "\n",
    "def benchmark_new_parquet(datalab: Datalab, path: Path) -> Dict[str, Union[str, float]]:\n",
    "    \"\"\"Benchmarks the new secure save/load method.\"\"\"\n",
    "    # Time the save operation\n",
    "    start_save: float = time.perf_counter()\n",
    "    datalab.save(str(path), force=True)\n",
    "    end_save: float = time.perf_counter()\n",
    "\n",
    "    # Time the load operation\n",
    "    start_load: float = time.perf_counter()\n",
    "    _ = Datalab.load(str(path))\n",
    "    end_load: float = time.perf_counter()\n",
    "\n",
    "    # Measure total directory size\n",
    "    total_size: int = sum(f.stat().st_size for f in path.glob('**/*') if f.is_file())\n",
    "    size_mb: float = total_size / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        \"Format\": \"New (Parquet)\",\n",
    "        \"Save Time (s)\": end_save - start_save,\n",
    "        \"Load Time (s)\": end_load - start_load,\n",
    "        \"Size on Disk (MB)\": size_mb,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmarks\n",
    "\n",
    "Now we execute the benchmark functions and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running benchmarks...\")\n",
    "\n",
    "# Create Path objects for the benchmark directories\n",
    "legacy_path = Path(\"legacy_lab_bench\")\n",
    "new_path = Path(\"new_lab_bench\")\n",
    "\n",
    "legacy_results: Dict[str, Union[str, float]] = benchmark_legacy_pickle(lab, legacy_path)\n",
    "new_results: Dict[str, Union[str, float]] = benchmark_new_parquet(lab, new_path)\n",
    "\n",
    "# Clean up benchmark directories\n",
    "shutil.rmtree(\"legacy_lab_bench\")\n",
    "shutil.rmtree(\"new_lab_bench\")\n",
    "\n",
    "print(\"Benchmarks complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results\n",
    "\n",
    "Finally, we'll format the results into a clear table and calculate the performance improvement factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build results DataFrame\n",
    "results_df: pd.DataFrame = pd.DataFrame([legacy_results, new_results])\n",
    "\n",
    "# Calculate performance and size ratios\n",
    "try:\n",
    "    # A factor > 1 means the new method is slower\n",
    "    save_ratio: float = new_results[\"Save Time (s)\"] / legacy_results[\"Save Time (s)\"]\n",
    "    load_ratio: float = new_results[\"Load Time (s)\"] / legacy_results[\"Load Time (s)\"]\n",
    "    \n",
    "    # A factor > 1 means the new method produces smaller files\n",
    "    size_ratio: float = legacy_results[\"Size on Disk (MB)\"] / new_results[\"Size on Disk (MB)\"]\n",
    "except ZeroDivisionError:\n",
    "    save_ratio = float(\"inf\")\n",
    "    load_ratio = float(\"inf\")\n",
    "    size_ratio = float(\"inf\")\n",
    "\n",
    "# Style the DataFrame for better presentation\n",
    "styled_df: Styler = (\n",
    "    results_df.style\n",
    "    .format({\n",
    "        \"Save Time (s)\": \"{:.3f}\",\n",
    "        \"Load Time (s)\": \"{:.3f}\",\n",
    "        \"Size on Disk (MB)\": \"{:.2f}\"\n",
    "    })\n",
    "    .set_caption(\"Serialization Performance Comparison\")\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "print(\"--- Benchmark Results ---\")\n",
    "display(styled_df)\n",
    "\n",
    "# --- Conclusion and Analysis ---\n",
    "print(\"\\n--- Conclusion ---\")\n",
    "print(\"✅ Security: The new Parquet engine eliminates the RCE vulnerability of pickle.\")\n",
    "print(f\"✅ Disk Efficiency: Parquet files are {size_ratio:.1f}x smaller on disk.\")\n",
    "\n",
    "# Provide clearer, more intuitive performance results\n",
    "if save_ratio > 1:\n",
    "    print(f\"🔴 Save Speed: Parquet is {save_ratio:.1f}x slower than pickle for this task.\")\n",
    "else:\n",
    "    # If ratio is 0.5, it's 2x faster (1/0.5)\n",
    "    print(f\"🟢 Save Speed: Parquet is {1/save_ratio:.1f}x faster than pickle for this task.\")\n",
    "\n",
    "if load_ratio > 1:\n",
    "    print(f\"🔴 Load Speed: Parquet is {load_ratio:.1f}x slower than pickle for this task.\")\n",
    "else:\n",
    "    print(f\"🟢 Load Speed: Parquet is {1/load_ratio:.1f}x faster than pickle for this task.\")\n",
    "\n",
    "# Add the more nuanced explanation if Parquet is slower\n",
    "if save_ratio > 1 or load_ratio > 1:\n",
    "    print(\"\\n💡 **Performance Nuance:** The speed decrease is expected and highlights the difference in design.\")\n",
    "    print(\"   - **Pickle** is fast here because it performs a simple, raw memory dump. It's a blunt instrument with minimal overhead.\")\n",
    "    print(\"   - **Parquet** is more deliberate. It's a sophisticated format that incurs a higher upfront cost to do more work: analyzing the data schema, restructuring data into columns, and applying intelligent compression. This provides its significant advantages in security, file size, and cross-platform compatibility, especially at scale.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
