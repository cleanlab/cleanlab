{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Duplicate Detection with Datalab\n",
    "\n",
    "This tutorial demonstrates the enhanced duplicate detection capabilities in Cleanlab's Datalab, including exact duplicate detection, near-duplicate detection with configurable similarity thresholds, and performance optimization techniques.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Understand the difference between exact and near-duplicate detection\n",
    "- Use similarity thresholds for fine-tuned cosine similarity matching\n",
    "- Compare performance across different dataset sizes\n",
    "- Apply duplicate detection to real-world scenarios (text, embeddings)\n",
    "- Optimize detection for large datasets\n",
    "\n",
    "The enhanced duplicate detection supports:\n",
    "- **Exact duplicates**: Identical examples (distance = 0)\n",
    "- **Near duplicates**: Highly similar examples based on configurable thresholds\n",
    "- **Cosine similarity thresholds**: Direct similarity control (0-1 range)\n",
    "- **Multiple distance metrics**: Euclidean, cosine, manhattan, etc.\n",
    "- **Scalable detection**: Optimized for datasets up to 1M+ rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Package installation (hidden on docs website).\n",
    "dependencies = [\"cleanlab\", \"matplotlib\", \"scikit-learn\", \"numpy\", \"pandas\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    dependencies_test = [dependency.split('>')[0] if '>' in dependency \n",
    "                         else dependency.split('<')[0] if '<' in dependency \n",
    "                         else dependency.split('=')[0] for dependency in dependencies]\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies_test:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "from cleanlab import Datalab\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Duplicate Detection: Exact vs Near Duplicates\n",
    "\n",
    "Let's start with a simple example to understand the difference between exact and near-duplicate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_duplicate_dataset():\n",
    "    \"\"\"Create a dataset with exact and near duplicates for demonstration.\"\"\"\n",
    "    \n",
    "    # Create base data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    n_features = 5\n",
    "    \n",
    "    # Generate random base data\n",
    "    X_base = np.random.randn(n_samples, n_features)\n",
    "    y_base = np.random.choice(['A', 'B', 'C'], size=n_samples)\n",
    "    \n",
    "    # Add exact duplicates\n",
    "    exact_duplicate_indices = [10, 25, 40]  # Duplicate these examples\n",
    "    X_exact_dups = X_base[exact_duplicate_indices].copy()\n",
    "    y_exact_dups = y_base[exact_duplicate_indices].copy()\n",
    "    \n",
    "    # Add near duplicates (very small noise)\n",
    "    near_duplicate_indices = [15, 30, 45]\n",
    "    X_near_dups = X_base[near_duplicate_indices].copy()\n",
    "    X_near_dups += np.random.normal(0, 0.01, X_near_dups.shape)  # Add tiny noise\n",
    "    y_near_dups = y_base[near_duplicate_indices].copy()\n",
    "    \n",
    "    # Combine all data\n",
    "    X_combined = np.vstack([X_base, X_exact_dups, X_near_dups])\n",
    "    y_combined = np.hstack([y_base, y_exact_dups, y_near_dups])\n",
    "    \n",
    "    return X_combined, y_combined, exact_duplicate_indices, near_duplicate_indices\n",
    "\n",
    "X, y, exact_indices, near_indices = create_duplicate_dataset()\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Exact duplicate pairs: {[(i, i+len(X)//2) for i in exact_indices]}\")\n",
    "print(f\"Near duplicate pairs: {[(i, i+len(X)//2+len(exact_indices)) for i in near_indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Detecting Only Exact Duplicates\n",
    "\n",
    "First, let's detect only exact duplicates using the `exact_duplicates_only=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create Datalab instance\n",
    "data_dict = {\"features\": X, \"labels\": y}\n",
    "lab_exact = Datalab(data_dict, label_name=\"labels\")\n",
    "\n",
    "# Detect only exact duplicates\n",
    "lab_exact.find_issues(\n",
    "    features=X,\n",
    "    issue_types={\n",
    "        \"near_duplicate\": {\n",
    "            \"exact_duplicates_only\": True,\n",
    "            \"metric\": \"euclidean\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=== EXACT DUPLICATES ONLY ===\")\n",
    "exact_issues = lab_exact.get_issues(\"near_duplicate\")\n",
    "exact_duplicates = exact_issues[exact_issues[\"is_near_duplicate_issue\"]]\n",
    "print(f\"Found {len(exact_duplicates)} exact duplicate examples\")\n",
    "print(\"\\nExact duplicate examples:\")\n",
    "print(exact_duplicates[[\"near_duplicate_score\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Detecting Near Duplicates with Default Threshold\n",
    "\n",
    "Now let's detect both exact and near duplicates using the default distance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create new Datalab instance for near duplicates\n",
    "lab_near = Datalab(data_dict, label_name=\"labels\")\n",
    "\n",
    "# Detect near duplicates with default threshold\n",
    "lab_near.find_issues(\n",
    "    features=X,\n",
    "    issue_types={\n",
    "        \"near_duplicate\": {\n",
    "            \"exact_duplicates_only\": False,\n",
    "            \"metric\": \"euclidean\",\n",
    "            \"threshold\": 0.1  # More permissive threshold\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=== NEAR DUPLICATES (DEFAULT THRESHOLD) ===\")\n",
    "near_issues = lab_near.get_issues(\"near_duplicate\")\n",
    "all_duplicates = near_issues[near_issues[\"is_near_duplicate_issue\"]]\n",
    "print(f\"Found {len(all_duplicates)} near duplicate examples\")\n",
    "print(\"\\nNear duplicate examples (lowest scores = most similar):\")\n",
    "print(all_duplicates[[\"near_duplicate_score\"]].sort_values(\"near_duplicate_score\").head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cosine Similarity with Configurable Thresholds\n",
    "\n",
    "The enhanced duplicate detection supports direct similarity thresholds for cosine similarity, which is particularly useful for text embeddings and high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_text_embeddings_dataset():\n",
    "    \"\"\"Create a text dataset with embeddings for similarity testing.\"\"\"\n",
    "    \n",
    "    # Sample text data with intentional duplicates and near-duplicates\n",
    "    texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"A fast brown fox leaps over a sleepy dog\",  # Near duplicate\n",
    "        \"The weather is sunny today\",\n",
    "        \"Machine learning is fascinating\",\n",
    "        \"Deep learning models are powerful\",\n",
    "        \"The quick brown fox jumps over the lazy dog\",  # Exact duplicate\n",
    "        \"Today the weather is sunny\",  # Near duplicate\n",
    "        \"Artificial intelligence is the future\",\n",
    "        \"Python is a great programming language\",\n",
    "        \"Data science involves statistics and programming\",\n",
    "        \"Machine learning is really fascinating\",  # Near duplicate\n",
    "        \"The weather is sunny today\",  # Exact duplicate\n",
    "    ]\n",
    "    \n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "    embeddings = vectorizer.fit_transform(texts).toarray()\n",
    "    \n",
    "    # Create labels\n",
    "    labels = ['text'] * len(texts)\n",
    "    \n",
    "    return embeddings, labels, texts\n",
    "\n",
    "text_embeddings, text_labels, original_texts = create_text_embeddings_dataset()\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Number of texts: {len(original_texts)}\")\n",
    "print(\"\\nSample texts:\")\n",
    "for i, text in enumerate(original_texts[:5]):\n",
    "    print(f\"{i}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 High Similarity Threshold (95%)\n",
    "\n",
    "Let's detect only very similar texts using a 95% cosine similarity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create Datalab for text similarity\n",
    "text_data = {\"embeddings\": text_embeddings, \"labels\": text_labels}\n",
    "lab_text_high = Datalab(text_data, label_name=\"labels\")\n",
    "\n",
    "# Detect duplicates with 95% similarity threshold\n",
    "lab_text_high.find_issues(\n",
    "    features=text_embeddings,\n",
    "    issue_types={\n",
    "        \"near_duplicate\": {\n",
    "            \"metric\": \"cosine\",\n",
    "            \"similarity_threshold\": 0.95  # 95% similarity\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=== TEXT DUPLICATES (95% SIMILARITY) ===\")\n",
    "text_issues_high = lab_text_high.get_issues(\"near_duplicate\")\n",
    "high_sim_duplicates = text_issues_high[text_issues_high[\"is_near_duplicate_issue\"]]\n",
    "print(f\"Found {len(high_sim_duplicates)} highly similar texts\")\n",
    "\n",
    "if len(high_sim_duplicates) > 0:\n",
    "    print(\"\\nHighly similar text pairs:\")\n",
    "    for idx in high_sim_duplicates.index:\n",
    "        score = high_sim_duplicates.loc[idx, \"near_duplicate_score\"]\n",
    "        print(f\"Index {idx} (score: {score:.4f}): {original_texts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Medium Similarity Threshold (80%)\n",
    "\n",
    "Now let's use a more permissive 80% similarity threshold to catch more near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create new Datalab for medium similarity\n",
    "lab_text_medium = Datalab(text_data, label_name=\"labels\")\n",
    "\n",
    "# Detect duplicates with 80% similarity threshold\n",
    "lab_text_medium.find_issues(\n",
    "    features=text_embeddings,\n",
    "    issue_types={\n",
    "        \"near_duplicate\": {\n",
    "            \"metric\": \"cosine\",\n",
    "            \"similarity_threshold\": 0.80  # 80% similarity\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"=== TEXT DUPLICATES (80% SIMILARITY) ===\")\n",
    "text_issues_medium = lab_text_medium.get_issues(\"near_duplicate\")\n",
    "medium_sim_duplicates = text_issues_medium[text_issues_medium[\"is_near_duplicate_issue\"]]\n",
    "print(f\"Found {len(medium_sim_duplicates)} similar texts\")\n",
    "\n",
    "if len(medium_sim_duplicates) > 0:\n",
    "    print(\"\\nSimilar text pairs:\")\n",
    "    for idx in medium_sim_duplicates.index:\n",
    "        score = medium_sim_duplicates.loc[idx, \"near_duplicate_score\"]\n",
    "        print(f\"Index {idx} (score: {score:.4f}): {original_texts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Similarity Threshold Comparison\n",
    "\n",
    "Let's compare the results across different similarity thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different similarity thresholds\n",
    "thresholds = [0.99, 0.95, 0.90, 0.85, 0.80, 0.75]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    lab_temp = Datalab(text_data, label_name=\"labels\")\n",
    "    lab_temp.find_issues(\n",
    "        features=text_embeddings,\n",
    "        issue_types={\n",
    "            \"near_duplicate\": {\n",
    "                \"metric\": \"cosine\",\n",
    "                \"similarity_threshold\": threshold\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    issues = lab_temp.get_issues(\"near_duplicate\")\n",
    "    num_duplicates = len(issues[issues[\"is_near_duplicate_issue\"]])\n",
    "    results.append((threshold, num_duplicates))\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results, columns=[\"Similarity Threshold\", \"Duplicates Found\"])\n",
    "print(\"=== SIMILARITY THRESHOLD COMPARISON ===\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(comparison_df[\"Similarity Threshold\"], comparison_df[\"Duplicates Found\"], \n",
    "         marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel(\"Cosine Similarity Threshold\")\n",
    "plt.ylabel(\"Number of Duplicates Found\")\n",
    "plt.title(\"Duplicate Detection vs Similarity Threshold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0.74, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Scaling and Optimization\n",
    "\n",
    "Let's test the performance of duplicate detection across different dataset sizes and explore optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def benchmark_duplicate_detection(sizes=[100, 500, 1000, 2000]):\n",
    "    \"\"\"Benchmark duplicate detection performance across dataset sizes.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nTesting dataset size: {size}\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        np.random.seed(42)\n",
    "        X_test = np.random.randn(size, 50)  # 50-dimensional features\n",
    "        y_test = np.random.choice(['A', 'B', 'C'], size=size)\n",
    "        \n",
    "        # Add some duplicates\n",
    "        n_duplicates = min(10, size // 10)\n",
    "        duplicate_indices = np.random.choice(size, n_duplicates, replace=False)\n",
    "        for i, dup_idx in enumerate(duplicate_indices):\n",
    "            if dup_idx + size // 2 < size:\n",
    "                X_test[dup_idx + size // 2] = X_test[dup_idx]  # Create exact duplicate\n",
    "        \n",
    "        data_test = {\"features\": X_test, \"labels\": y_test}\n",
    "        \n",
    "        # Benchmark with cosine similarity\n",
    "        lab_bench = Datalab(data_test, label_name=\"labels\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        lab_bench.find_issues(\n",
    "            features=X_test,\n",
    "            issue_types={\n",
    "                \"near_duplicate\": {\n",
    "                    \"metric\": \"cosine\",\n",
    "                    \"similarity_threshold\": 0.95,\n",
    "                    \"k\": min(10, size - 1)  # Adaptive k\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        issues = lab_bench.get_issues(\"near_duplicate\")\n",
    "        num_found = len(issues[issues[\"is_near_duplicate_issue\"]])\n",
    "        \n",
    "        results.append({\n",
    "            \"Dataset Size\": size,\n",
    "            \"Execution Time (s)\": round(execution_time, 3),\n",
    "            \"Duplicates Found\": num_found,\n",
    "            \"Time per Sample (ms)\": round(execution_time * 1000 / size, 3)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Time: {execution_time:.3f}s, Found: {num_found} duplicates\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_duplicate_detection([100, 500, 1000, 2000])\n",
    "print(\"\\n=== PERFORMANCE BENCHMARK ===\")\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot performance scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total execution time\n",
    "ax1.plot(benchmark_results[\"Dataset Size\"], benchmark_results[\"Execution Time (s)\"], \n",
    "         marker='o', linewidth=2, markersize=6, color='blue')\n",
    "ax1.set_xlabel(\"Dataset Size\")\n",
    "ax1.set_ylabel(\"Execution Time (seconds)\")\n",
    "ax1.set_title(\"Total Execution Time vs Dataset Size\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Time per sample\n",
    "ax2.plot(benchmark_results[\"Dataset Size\"], benchmark_results[\"Time per Sample (ms)\"], \n",
    "         marker='s', linewidth=2, markersize=6, color='red')\n",
    "ax2.set_xlabel(\"Dataset Size\")\n",
    "ax2.set_ylabel(\"Time per Sample (milliseconds)\")\n",
    "ax2.set_title(\"Time per Sample vs Dataset Size\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Application: Document Deduplication\n",
    "\n",
    "Let's apply the enhanced duplicate detection to a practical document deduplication scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_document_dataset():\n",
    "    \"\"\"Create a realistic document dataset with various types of duplicates.\"\"\"\n",
    "    \n",
    "    documents = [\n",
    "        # Research papers (with duplicates)\n",
    "        \"Machine learning algorithms for predictive analytics in healthcare systems\",\n",
    "        \"Deep neural networks applications in computer vision and image recognition\",\n",
    "        \"Natural language processing techniques for sentiment analysis in social media\",\n",
    "        \"Machine learning algorithms for predictive analytics in healthcare systems\",  # Exact\n",
    "        \"ML algorithms for predictive analytics in healthcare systems\",  # Near\n",
    "        \n",
    "        # News articles\n",
    "        \"Stock market reaches new highs amid economic recovery signals\",\n",
    "        \"Technology companies report strong quarterly earnings and revenue growth\",\n",
    "        \"Climate change impacts global agriculture and food security concerns\",\n",
    "        \"Stock market hits new peaks as economic recovery shows promising signs\",  # Near\n",
    "        \"Technology companies report strong quarterly earnings and revenue growth\",  # Exact\n",
    "        \n",
    "        # Product descriptions\n",
    "        \"High-performance laptop with 16GB RAM and 512GB SSD storage capacity\",\n",
    "        \"Wireless bluetooth headphones with noise cancellation and 20-hour battery life\",\n",
    "        \"Professional digital camera with 24MP sensor and 4K video recording\",\n",
    "        \"High performance laptop featuring 16GB RAM and 512GB SSD storage\",  # Near\n",
    "        \"Professional digital camera with 24MP sensor and 4K video recording\",  # Exact\n",
    "        \n",
    "        # Unique documents\n",
    "        \"Quantum computing breakthrough promises revolutionary computational capabilities\",\n",
    "        \"Renewable energy innovations drive sustainable development initiatives worldwide\",\n",
    "        \"Artificial intelligence ethics guidelines for responsible AI development\",\n",
    "        \"Space exploration missions reveal new insights about planetary formation\",\n",
    "        \"Biotechnology advances enable personalized medicine treatments for patients\"\n",
    "    ]\n",
    "    \n",
    "    # Create embeddings using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=200, \n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)  # Include bigrams for better similarity detection\n",
    "    )\n",
    "    \n",
    "    embeddings = vectorizer.fit_transform(documents).toarray()\n",
    "    \n",
    "    # Create categories\n",
    "    categories = (\n",
    "        ['research'] * 5 + \n",
    "        ['news'] * 5 + \n",
    "        ['product'] * 5 + \n",
    "        ['misc'] * 5\n",
    "    )\n",
    "    \n",
    "    return embeddings, categories, documents\n",
    "\n",
    "doc_embeddings, doc_categories, documents = create_document_dataset()\n",
    "print(f\"Document dataset: {len(documents)} documents, {doc_embeddings.shape[1]} features\")\n",
    "print(f\"Categories: {np.unique(doc_categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Multi-Threshold Document Analysis\n",
    "\n",
    "Let's analyze the document dataset using multiple similarity thresholds to understand the duplicate landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_documents_at_threshold(embeddings, categories, documents, threshold):\n",
    "    \"\"\"Analyze document duplicates at a specific similarity threshold.\"\"\"\n",
    "    \n",
    "    doc_data = {\"embeddings\": embeddings, \"categories\": categories}\n",
    "    lab_docs = Datalab(doc_data, label_name=\"categories\")\n",
    "    \n",
    "    lab_docs.find_issues(\n",
    "        features=embeddings,\n",
    "        issue_types={\n",
    "            \"near_duplicate\": {\n",
    "                \"metric\": \"cosine\",\n",
    "                \"similarity_threshold\": threshold\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    issues = lab_docs.get_issues(\"near_duplicate\")\n",
    "    duplicates = issues[issues[\"is_near_duplicate_issue\"]].sort_values(\"near_duplicate_score\")\n",
    "    \n",
    "    print(f\"\\n=== SIMILARITY THRESHOLD: {threshold:.1%} ===\")\n",
    "    print(f\"Found {len(duplicates)} duplicate documents\")\n",
    "    \n",
    "    if len(duplicates) > 0:\n",
    "        print(\"\\nDuplicate documents:\")\n",
    "        for idx in duplicates.index:\n",
    "            score = duplicates.loc[idx, \"near_duplicate_score\"]\n",
    "            category = doc_categories[idx]\n",
    "            print(f\"\\n[{idx}] Score: {score:.4f} | Category: {category}\")\n",
    "            print(f\"Text: {documents[idx][:80]}...\")\n",
    "    \n",
    "    return len(duplicates), duplicates\n",
    "\n",
    "# Analyze at different thresholds\n",
    "thresholds_to_test = [0.99, 0.90, 0.80, 0.70]\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    num_found, _ = analyze_documents_at_threshold(\n",
    "        doc_embeddings, doc_categories, documents, threshold\n",
    "    )\n",
    "    threshold_results.append((threshold, num_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detailed Duplicate Analysis\n",
    "\n",
    "Let's examine the detected duplicates in detail and calculate pairwise similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Use optimal threshold for detailed analysis\n",
    "optimal_threshold = 0.85\n",
    "doc_data = {\"embeddings\": doc_embeddings, \"categories\": doc_categories}\n",
    "lab_final = Datalab(doc_data, label_name=\"categories\")\n",
    "\n",
    "lab_final.find_issues(\n",
    "    features=doc_embeddings,\n",
    "    issue_types={\n",
    "        \"near_duplicate\": {\n",
    "            \"metric\": \"cosine\",\n",
    "            \"similarity_threshold\": optimal_threshold\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "final_issues = lab_final.get_issues(\"near_duplicate\")\n",
    "final_duplicates = final_issues[final_issues[\"is_near_duplicate_issue\"]].sort_values(\"near_duplicate_score\")\n",
    "\n",
    "print(f\"=== DETAILED DUPLICATE ANALYSIS (Threshold: {optimal_threshold:.1%}) ===\")\n",
    "print(f\"Total duplicates found: {len(final_duplicates)}\")\n",
    "\n",
    "# Calculate pairwise similarities for all duplicate pairs\n",
    "if len(final_duplicates) > 0:\n",
    "    print(\"\\n=== PAIRWISE SIMILARITY ANALYSIS ===\")\n",
    "    \n",
    "    # Get duplicate sets information\n",
    "    duplicate_info = lab_final.get_info(\"near_duplicate\")\n",
    "    duplicate_sets = duplicate_info.get(\"near_duplicate_sets\", [])\n",
    "    \n",
    "    # Find actual duplicate pairs\n",
    "    duplicate_pairs = []\n",
    "    for i, dup_set in enumerate(duplicate_sets):\n",
    "        if len(dup_set) > 0:  # This example has duplicates\n",
    "            for j in dup_set:\n",
    "                if i < j:  # Avoid duplicate pairs\n",
    "                    similarity = cosine_similarity(\n",
    "                        doc_embeddings[i:i+1], \n",
    "                        doc_embeddings[j:j+1]\n",
    "                    )[0, 0]\n",
    "                    duplicate_pairs.append((i, j, similarity))\n",
    "    \n",
    "    # Display duplicate pairs\n",
    "    for i, (idx1, idx2, similarity) in enumerate(sorted(duplicate_pairs, key=lambda x: x[2], reverse=True)):\n",
    "        print(f\"\\n--- Duplicate Pair {i+1} (Similarity: {similarity:.3f}) ---\")\n",
    "        print(f\"[{idx1}] {documents[idx1]}\")\n",
    "        print(f\"[{idx2}] {documents[idx2]}\")\n",
    "        print(f\"Categories: {doc_categories[idx1]} vs {doc_categories[idx2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Recommendations\n",
    "\n",
    "Based on our experiments, here are key recommendations for using the enhanced duplicate detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=== DUPLICATE DETECTION BEST PRACTICES ===\")\n",
    "print()\n",
    "print(\"1. CHOOSING THE RIGHT METRIC:\")\n",
    "print(\"   • Cosine similarity: Best for text embeddings, high-dimensional sparse data\")\n",
    "print(\"   • Euclidean distance: Good for dense numerical features, image embeddings\")\n",
    "print(\"   • Manhattan distance: Robust to outliers, good for mixed data types\")\n",
    "print()\n",
    "print(\"2. SIMILARITY THRESHOLD GUIDELINES:\")\n",
    "print(\"   • 0.95-0.99: Very strict, catches only near-identical content\")\n",
    "print(\"   • 0.85-0.95: Moderate, good balance for most applications\")\n",
    "print(\"   • 0.70-0.85: Permissive, may catch semantically related content\")\n",
    "print(\"   • <0.70: Very permissive, high false positive rate\")\n",
    "print()\n",
    "print(\"3. PERFORMANCE OPTIMIZATION:\")\n",
    "print(\"   • Use exact_duplicates_only=True for preprocessing steps\")\n",
    "print(\"   • Reduce k for large datasets (k=5-10 usually sufficient)\")\n",
    "print(\"   • Consider batch processing for datasets >50k rows\")\n",
    "print(\"   • Use sparse feature representations when possible\")\n",
    "print()\n",
    "print(\"4. VALIDATION STRATEGIES:\")\n",
    "print(\"   • Test multiple thresholds on a sample of your data\")\n",
    "print(\"   • Manually review detected pairs to calibrate thresholds\")\n",
    "print(\"   • Consider domain-specific similarity requirements\")\n",
    "print(\"   • Monitor false positive/negative rates\")\n",
    "print()\n",
    "print(\"5. COMMON USE CASES:\")\n",
    "print(\"   • Data cleaning: Use high thresholds (0.95+) to remove obvious duplicates\")\n",
    "print(\"   • Content deduplication: Medium thresholds (0.85-0.95) for similar content\")\n",
    "print(\"   • Similarity search: Lower thresholds (0.70-0.85) for related items\")\n",
    "print(\"   • Quality assurance: exact_duplicates_only for data validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Configuration Examples\n",
    "\n",
    "Here are some advanced configuration examples for specific use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example configurations for different scenarios\n",
    "\n",
    "print(\"=== ADVANCED CONFIGURATION EXAMPLES ===\")\n",
    "print()\n",
    "\n",
    "# Configuration 1: Strict data cleaning\n",
    "print(\"1. STRICT DATA CLEANING:\")\n",
    "print(\"   Purpose: Remove obvious duplicates during data preprocessing\")\n",
    "strict_config = {\n",
    "    \"near_duplicate\": {\n",
    "        \"exact_duplicates_only\": True,\n",
    "        \"metric\": \"euclidean\"\n",
    "    }\n",
    "}\n",
    "print(f\"   Config: {strict_config}\")\n",
    "print()\n",
    "\n",
    "# Configuration 2: Text content deduplication\n",
    "print(\"2. TEXT CONTENT DEDUPLICATION:\")\n",
    "print(\"   Purpose: Find similar articles, documents, or web content\")\n",
    "text_config = {\n",
    "    \"near_duplicate\": {\n",
    "        \"metric\": \"cosine\",\n",
    "        \"similarity_threshold\": 0.88,\n",
    "        \"k\": 15\n",
    "    }\n",
    "}\n",
    "print(f\"   Config: {text_config}\")\n",
    "print()\n",
    "\n",
    "# Configuration 3: Image similarity detection\n",
    "print(\"3. IMAGE SIMILARITY DETECTION:\")\n",
    "print(\"   Purpose: Find similar images using deep learning embeddings\")\n",
    "image_config = {\n",
    "    \"near_duplicate\": {\n",
    "        \"metric\": \"cosine\",\n",
    "        \"similarity_threshold\": 0.92,\n",
    "        \"k\": 10\n",
    "    }\n",
    "}\n",
    "print(f\"   Config: {image_config}\")\n",
    "print()\n",
    "\n",
    "# Configuration 4: Product recommendation similarity\n",
    "print(\"4. PRODUCT RECOMMENDATION SIMILARITY:\")\n",
    "print(\"   Purpose: Find related products for recommendation systems\")\n",
    "product_config = {\n",
    "    \"near_duplicate\": {\n",
    "        \"metric\": \"cosine\",\n",
    "        \"similarity_threshold\": 0.75,\n",
    "        \"k\": 20\n",
    "    }\n",
    "}\n",
    "print(f\"   Config: {product_config}\")\n",
    "print()\n",
    "\n",
    "# Configuration 5: Large dataset optimization\n",
    "print(\"5. LARGE DATASET OPTIMIZATION:\")\n",
    "print(\"   Purpose: Efficient processing for datasets >10k rows\")\n",
    "large_config = {\n",
    "    \"near_duplicate\": {\n",
    "        \"metric\": \"cosine\",\n",
    "        \"similarity_threshold\": 0.90,\n",
    "        \"k\": 5,  # Reduced k for performance\n",
    "        \"threshold\": 0.1  # Fallback for non-cosine metrics\n",
    "    }\n",
    "}\n",
    "print(f\"   Config: {large_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Monitoring and Troubleshooting\n",
    "\n",
    "Here's how to monitor performance and troubleshoot common issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def demonstrate_error_handling():\n",
    "    \"\"\"Demonstrate common errors and how to handle them.\"\"\"\n",
    "    \n",
    "    print(\"=== COMMON ERRORS AND SOLUTIONS ===\")\n",
    "    print()\n",
    "    \n",
    "    # Error 1: Invalid similarity threshold\n",
    "    print(\"1. INVALID SIMILARITY THRESHOLD:\")\n",
    "    try:\n",
    "        test_data = {\"features\": np.random.randn(10, 5), \"labels\": ['A'] * 10}\n",
    "        lab_error = Datalab(test_data, label_name=\"labels\")\n",
    "        lab_error.find_issues(\n",
    "            features=test_data[\"features\"],\n",
    "            issue_types={\n",
    "                \"near_duplicate\": {\n",
    "                    \"metric\": \"cosine\",\n",
    "                    \"similarity_threshold\": 1.5  # Invalid: > 1\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(\"   Solution: Use similarity_threshold between 0 and 1\")\n",
    "    print()\n",
    "    \n",
    "    # Error 2: k too large\n",
    "    print(\"2. K TOO LARGE FOR DATASET:\")\n",
    "    try:\n",
    "        small_data = {\"features\": np.random.randn(5, 3), \"labels\": ['A'] * 5}\n",
    "        lab_error2 = Datalab(small_data, label_name=\"labels\")\n",
    "        lab_error2.find_issues(\n",
    "            features=small_data[\"features\"],\n",
    "            issue_types={\n",
    "                \"near_duplicate\": {\n",
    "                    \"k\": 10  # Too large for 5 samples\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"   Error: {str(e)[:80]}...\")\n",
    "        print(\"   Solution: Use k < dataset_size (recommended: k <= 10)\")\n",
    "    print()\n",
    "    \n",
    "    # Error 3: Empty dataset\n",
    "    print(\"3. EMPTY DATASET:\")\n",
    "    try:\n",
    "        empty_data = {\"features\": np.array([]).reshape(0, 5), \"labels\": []}\n",
    "        lab_error3 = Datalab(empty_data, label_name=\"labels\")\n",
    "        lab_error3.find_issues(\n",
    "            features=empty_data[\"features\"],\n",
    "            issue_types={\"near_duplicate\": {}}\n",
    "        )\n",
    "        print(\"   Handled gracefully - no error raised\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== PERFORMANCE TIPS ===\")\n",
    "    print(\"• Monitor memory usage for large datasets\")\n",
    "    print(\"• Use similarity_threshold instead of distance threshold for cosine metric\")\n",
    "    print(\"• Consider exact_duplicates_only=True for initial data cleaning\")\n",
    "    print(\"• Reduce k for faster processing on large datasets\")\n",
    "    print(\"• Profile your code to identify bottlenecks\")\n",
    "\n",
    "demonstrate_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated the enhanced duplicate detection capabilities in Cleanlab's Datalab, including:\n",
    "\n",
    "- **Exact duplicate detection** for data cleaning and validation\n",
    "- **Configurable similarity thresholds** for fine-tuned control\n",
    "- **Performance optimization** techniques for large datasets\n",
    "- **Real-world applications** like document deduplication\n",
    "- **Best practices** for different use cases\n",
    "\n",
    "The enhanced duplicate detection provides powerful tools for:\n",
    "- **Data quality improvement**: Remove exact duplicates during preprocessing\n",
    "- **Content deduplication**: Find similar articles, documents, or media\n",
    "- **Similarity search**: Identify related items for recommendations\n",
    "- **Quality assurance**: Validate data integrity and consistency\n",
    "\n",
    "For more advanced usage and customization options, check out the [Datalab documentation](https://docs.cleanlab.ai/stable/cleanlab/datalab/guide/issue_type_description.html#near-duplicate-issue) and explore the various configuration parameters available.\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different similarity thresholds on your own data\n",
    "- Try different distance metrics based on your data type\n",
    "- Consider batch processing for very large datasets\n",
    "- Integrate duplicate detection into your data preprocessing pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}